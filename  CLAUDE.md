# èª­ã‚€ (Yomu) â€” Japanese Reader PWA
## Claude Code Project Brief

---

## What We're Building

A mobile PWA (Progressive Web App) for reading Japanese books. The core use case: I'm reading a Japanese novel physically or on my phone, I hit an unknown kanji or vocab word, I open the app, photograph the page, tap the word I want, and instantly get the reading, meaning, and my personal study data â€” without breaking my reading flow.

This replaces the painful workflow of drawing kanji on Google Translate or switching between multiple apps.

---

## Core User Flow

1. Open app on iPhone (installed to home screen as PWA)
2. Tap camera button â†’ capture page of Japanese text
3. App processes image and overlays **tap targets** on all detected kanji/vocab
4. User taps a specific word
5. Bottom sheet slides up showing:
   - Reading (furigana)
   - Meaning in English
   - WaniKani SRS status badge (Apprentice / Guru / Master / Enlightened / Not in WK)
   - Whether it exists in my Anki deck
   - Breakdown of individual kanji components if it's a multi-kanji vocab word
6. One tap to add to Anki (if not already in deck)

---

## Tech Stack

### Frontend
- **Vanilla JS + HTML/CSS** â€” keep it simple, no framework needed for a PWA
- Installable on iPhone via Safari "Add to Home Screen"
- Camera access via `getUserMedia` or `<input type="file" accept="image/*" capture="environment">`
- Responsive, mobile-first design
- Dark theme, clean and minimal â€” designed for focused reading sessions

### APIs

**1. Google Cloud Vision API** (OCR + bounding boxes)
- Used for detecting all Japanese text on the page with pixel-accurate bounding boxes
- This is critical â€” we need positions so we can overlay tap targets on the photo
- Use the `DOCUMENT_TEXT_DETECTION` feature which returns per-character and per-word bounding boxes
- Handles vertical Japanese text well
- Endpoint: `https://vision.googleapis.com/v1/images:annotate`
- API key stored in a local `.env` file (never hardcoded)

**2. Anthropic Claude API** (meaning + context)
- Once user taps a word, send that word to Claude for:
  - Reading (hiragana/furigana)
  - English meaning
  - Part of speech
  - Example sentence from context (if we pass the surrounding text)
  - Breakdown of component kanji (for vocab words)
- Use `claude-sonnet-4-20250514`
- System prompt should establish that responses are for a Japanese learner at WaniKani level 24 (roughly 600-700 kanji known)
- Return structured JSON for easy rendering

**3. WaniKani API** (SRS status)
- On app load, fetch user's assignments from WaniKani API
- Cache locally (refresh once per day is fine)
- When a kanji/word is looked up, check cached data to show SRS level
- Endpoint: `https://api.wanikani.com/v2/assignments`
- Personal API token stored in `.env`

**4. AnkiConnect** (Anki integration)
- AnkiConnect runs as a local server on desktop at `http://localhost:8765`
- When on same WiFi as desktop, the app can:
  - Check if a word already exists in the deck called "ALL KANJI COMBINED"
  - Add new cards using the following exact format:

**Front:** just the kanji/vocab word, nothing else (e.g. `å‹‰å¼·`)

**Back:**
  - Reading in hiragana (e.g. `ã¹ã‚“ãã‚‡ã†`)
  - Sentence 1: extracted from the photographed page, with furigana in Anki ruby format (e.g. `å½¼å¥³ã¯æ¯æ—¥å‹‰å¼·[ã¹ã‚“ãã‚‡ã†]ã—ã¦ã„ã‚‹ã€‚`)
  - Sentence 2: generated by Claude, natural, similar difficulty to source material, different context than Sentence 1

- Furigana format is Anki ruby style: `æ¼¢å­—[ã‚ˆã¿ã‹ãŸ]` â€” only add furigana to kanji characters, not kana
- If a clean sentence can't be extracted from the page (word at edge of page, etc.), fall back to two Claude-generated sentences
- Claude should generate Sentence 2 to use the word in a meaningfully different context than Sentence 1
- This feature should gracefully fail/hide if AnkiConnect is unreachable (e.g. when not on home WiFi)
- AnkiConnect API uses JSON-RPC style requests

---

## Key Technical Details

### Tap Target Overlay
This is the most important UX piece. After capturing a photo:
1. Send image to Google Vision API
2. Response includes `textAnnotations` with bounding boxes for each word/token
3. Display the photo in a scrollable container
4. Render semi-transparent tap target boxes over each detected word
5. User taps a box â†’ extract that word's text from the Vision response
6. Send to Claude for lookup

**Important**: Japanese text can be vertical (top-to-bottom, right-to-left columns) or horizontal. Google Vision handles both and returns correct bounding boxes either way.

### Kanji vs Vocab distinction
- Single kanji lookup â†’ show kanji meaning, reading, radical breakdown, WaniKani level
- Multi-kanji vocab word â†’ show full word reading + meaning, THEN break down each kanji individually
- Claude should handle this distinction based on what's tapped

### WaniKani Status Badges
Color-coded badges on the lookup result:
- ğŸ”´ **Not in WaniKani** â€” not part of WK curriculum
- ğŸŸ  **Apprentice** (SRS levels 1-4)
- ğŸŸ¡ **Guru** (SRS levels 5-6)
- ğŸ”µ **Master** (SRS level 7)
- ğŸŸ£ **Enlightened** (SRS level 8)
- âšª **Burned** (SRS level 9)
- â¬œ **Locked** â€” in WK curriculum but not yet unlocked at level 24

### AnkiConnect Graceful Degradation + Offline Queue
- On load, ping AnkiConnect silently
- If reachable: show "Add to Anki" button in bottom sheet, cards are added immediately
- If unreachable: show "Queue for Anki" button instead â€” card is saved to a local queue in localStorage
- Try to reconnect on each new lookup (user may have opened Anki or returned home)
- On every app open, silently check if AnkiConnect is reachable â€” if yes, automatically flush the queue in the background
- Show a small badge/indicator when there are queued cards pending sync
- Queue stores full card data: kanji, reading, meaning, example sentence, timestamp
- After successful flush, clear the queue and show a brief confirmation ("3 cards synced to Anki")

---

## Project Structure

```
yomu-app/
â”œâ”€â”€ index.html
â”œâ”€â”€ manifest.json          # PWA manifest
â”œâ”€â”€ sw.js                  # Service worker (basic caching)
â”œâ”€â”€ .env                   # API keys (gitignored)
â”œâ”€â”€ .env.example           # Template with key names
â”œâ”€â”€ css/
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ js/
â”‚   â”œâ”€â”€ app.js             # Main app logic
â”‚   â”œâ”€â”€ camera.js          # Camera capture handling
â”‚   â”œâ”€â”€ vision.js          # Google Vision API calls
â”‚   â”œâ”€â”€ claude.js          # Anthropic API calls
â”‚   â”œâ”€â”€ wanikani.js        # WaniKani API + local cache
â”‚   â””â”€â”€ anki.js            # AnkiConnect integration
â””â”€â”€ icons/                 # PWA icons (generate from a base icon)
```

---

## Environment Variables Needed

```
GOOGLE_VISION_API_KEY=
ANTHROPIC_API_KEY=
WANIKANI_API_TOKEN=
```

Note: Since this is a client-side PWA, API keys will be in the browser. For personal use this is acceptable. If this ever becomes a shared app, add a simple proxy server (e.g. a small Express server or Cloudflare Worker) to keep keys server-side.

---

## Design Direction

- **Dark theme** â€” easy on eyes during reading sessions
- **Minimal chrome** â€” the photo and tap targets should dominate the screen
- **Japanese typography** â€” use Noto Sans JP for any Japanese text rendering
- **Bottom sheet pattern** â€” lookup results slide up from bottom, dismissible by swipe or tap outside
- **Status colors** â€” WaniKani SRS levels use consistent color coding throughout
- Fast and snappy â€” lookup should feel nearly instant

---

## Constraints & Notes

- iOS Safari PWA limitations: no push notifications, limited background processing â€” that's fine
- AnkiConnect only works on same local network as desktop running Anki â€” this is acceptable
- WaniKani cache should persist in localStorage, refresh daily
- Keep Claude API calls focused â€” pass only the tapped word + a window of surrounding text for context, not the entire page
- Target: works great on iPhone 13 and newer

---

## What "Done" Looks Like for V1

- [ ] Camera capture works on iPhone
- [ ] Google Vision returns bounding boxes, tap targets render correctly over photo
- [ ] Tapping a word shows bottom sheet with reading + meaning from Claude
- [ ] WaniKani badge shows correct SRS status
- [ ] AnkiConnect "Add to Anki" button works when on home WiFi
- [ ] App is installable as PWA from Safari
- [ ] Graceful handling of all API failures

---

## Start Here

Begin with `index.html` and `js/vision.js`. Get the camera â†’ Google Vision â†’ tap target overlay loop working first. That's the hardest and most important part. Everything else (Claude lookup, WaniKani badges, Anki) layers on top once tap targets work.
